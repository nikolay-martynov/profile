= Scrum practice. Part 3: Sprint

_2009-04-08_

* It`s clear that use cases should be developed as another artifacts of sprints. It's important to produce complete high quality use case if product owner plans to have it implemented next sprint. If it does not meet user requirements then it is nothing. If it is too hard to implement and requires several man-years then it`s also of little help. So *use cases are very important*. Manager may want to use available resources to 100%. That means one man does his own thing. So manager may think that analyst has full responsibility for developing use case. That may work but most time it does not. Often use cases that were produced by one man are incomplete and have logic mistakes (in terms of user requirements). Often such use cases are too hard to implement or have technical mistakes. It is awful if technical mistakes are discovered only during implementation. Because programmer have to wait for a bug to be fixed just to discover a new bug. Implementation difficulties are not any better. Some requirement that is really not so important for user or some phrase that analyst had included in use case may make implementation nearly impossible. Product owner may expect this feature to be included in next version. But this may be impossible without strong reasons. And it is really disaster if testers (or worse, users) find that new feature does not address their needs or is not usable. This leads to single conclusion: use cases (same as specifications) should be not developed in old school way when one analyst does all job. *We should really follow practices of unified process *(Unified Software Development Process). At least to some degree. If it is possible then team (several members of team, product owner, client representative) should model use   case together. If there is no such possibility then at least reserve time to discuss use case. There are always should be testing of use case (and perhaps its acceptance by product owner). And discussion often saves time. Because inconsistencies and other problems are discovered earlier. Most of time discussion prevents from falling into write-test-fix-test-fix-test-fix cycle. Of course technical specialist like programmer should be involved in such activities. If something is too hard to implement then it is better to discover that earlier. If that thing is not too important for users and can be rephrased then this may save schedule and budget. Isn't it better to have product deployed than unimportant thing still being implemented?
* Scrum master is not police man that forces everyone to work. Scrum master is not boss who tells what to do and how. Scrum master is not one who just called so and nothing more. Actually scrum master have important role: scrum master saves team`s time and team implements more stories and/or achieves better quality. So what can scrum master really do? Answer is simple: keep team focused. If there is some external intrusion try to handle it yourself so team keeps working. If team talks about fishing during daily meeting then drag attention to scrum activities. If there is burn-down chart then update it yourself. If there is demonstration soon then reserve room yourself. *Scrum master can really help team to achieve higher velocity and better quality. *
* It may seem like having definition of done is not so important. Analyst may say during daily meeting "I have done with this use case and will do that". So says programmer: "I have done with that feature and will start with that one". But there are several problems. As I have said earlier, use cases have to be tested. It is clear that new implemented features should be tested too. And it is more: new feature may be implemented with unacceptable quality (yeah yeah, no pair programming). When testers find bugs in use cases and implementation does task/story move back to "in progress"? And one who originally was responsible for it take care of that bug? Does then test happen again? How much time is there between "I have done with that" and "I found bugs in that"? Does one who is responsible for buggy item interrupts current work on next item and fixes bugs? Or is there a dedicated week when bugs are fixed? Answers to these questions greatly  affect product quality and team`s velocity. *If team want to be faster and do better, then it really should have definition of done*. Universal definition is: something is done when testing says that it is done. Of course this is all about being agile. For example, for use cases product owner`s accepting may be required for "done". In practice for use cases this may look like this: several people discuss user requirement and scenario; analyst who name is on task card summarizes all that in a document; when he (task owner) is done he tells team about that, underlines own name on task card and write down two more names - programmer who will test and another analyst or tester who also will test final document (if there is a third person who want to test then let it be so). At this time it is easy to see that task in on testing phase just by looking on task card. Also, it is clear how much time this testing will take (approximately)  because both testers said how much time they need and a task owner (first analyst) added time to fix bugs. That analyst goes to next item. But his name is first on task card and he is personally responsible for that task. Every daily meeting these testers have to report their progress with testing this use case. If testing takes too long task owner may nag or find another testers (sure he does not want team to say that he can not take care of task). When tester finishes he underlines his name on task card. If there are issues then they are reported to task owner immediately (of course he says on daily meeting that testing was done, issues were found and task owner were informed). Person A decides himself when to fix issues keeping in mind that he is personally responsible for this task to be complete due to sprint end. Once he fixes issues he takes care that testers know about that. Once testing accept use case then it is done. Same goes for code. If you do not use pair programming then you probably want one tester from programmers and another tester from testers (or analysts, that depends on available human resources at time task owner completes coding). Programmer who does testing may test for compliance  with coding standards. He may check that code does right thing in right way and nothing was forgotten. He may check that there are automatic tests written if they are required. He may check that solution follows decisions made by hive mind during planning (of course it may differ but team should know why it is so and should be aware of right way). In addition to improving quality this has another consequence. One who was testing code will be aware of it. It will be easier for him to maintain it. He will be aware of decisions made during implementation. This greatly helps to prevent isolation, wheel reinvention and inconsistencies. This should not be underestimated. Second tester checks compliance with use case, checks boundary conditions and so forth (of course there also should be general testing that is not feature focused, even if you have a lot of automated acceptance tests). Once again, when tester finds problem he reports immediately to task owner. When task card have all names underlined that means that feature in fully implemented and tested. Such testing is not really time-consuming. Practice shows that experienced team do testing same or next day. And most tasks are completed withing 1-2 daily meetings. This way users are less likely to face nasty message box with stack trace or some logic error.
* Another thing that may seem to be unimportant is team members co-location. Again *it is important for team to sit in one room*. And not one room with programmers, another room with analyst and third room with testers. One room means one room for whole team. That saves a lot of time when finding someone to test your task, when asking about testing progress, when reporting issues to task owner and another tester. That saves a lot of time and effort when programmer or tester need consultation from analysts. That saves a lot of time and effort when one asks question and any who knows answer helps him. Such communication is much more efficient than official meeting. What is also important, everyone knows what is happening. This all saves a lot of time.
* *A word on task cards*. A white board or wall with tasks can quickly turn into a mess where it is very hard to find some information. So discipline is important. Using my practice I can give following tips:
** Reserve less space for "not started" and "done" and more space for "in progress" section of white board.
** One task card should not close any piece of another card if they are in "in progress" section.
** Keep task cards related to single story grouped together.
** Do not put task numbers on a front page of cards. People will tell numbers on daily meetings and no one will understand what was done. If you need numbers then put them on back side of cards. (Also, scrum master should keep reporter saying "I was working on __insert task title here__ ..." instead of "I was working on this task" with finger pointing to card.)
** Split cards into sections. Each section for a different kind of information.
* *The next important thing is sprint schedule*. This is because sprint should end with complete version that will be demonstrated and perhaps given to customers. Practice shows that there should be time reserved for preparation activities. These include:
** Rearrange resources finish problematic stories.
** Finish fixing bugs for current stories.
** Do additional testing.
** Create an accompanying documentation for this version (change logs and another required documentation).
** Add additional automatic tests.
** Prepare for demonstration (choose what to demonstrate, prepare data, rehearse).
** Create a branch for this version and build distribution.
** Notify all interested people.
* All these activities require some time to complete them. So it is better to plan them than have no time for them. Every team member should know how much time left before sprint ends. Every team member should know if there is still time to make critical changes that potentially will break half of system. Every team member should know if it is time to take a story from "next" section or concentrate on bugs fixing and testing. One way to help with that is to have sprint schedule. From this schedule it should be clear where team currently is, how much time left and what team should concentrate on. This way it is less likely that team will not be ready to demonstrate a version and there will be no distribution because system does not work, new features are not complete, there are no documentation and there is no one who can demonstrate anything.
